{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Posao\\firebot-segmentation\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import time\n",
    "import string\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "from typing import List\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.rgb import mask2rgb\n",
    "from utils.prediction.evaluations import visualize, preload_image_data\n",
    "from utils.prediction.predict import Prediction\n",
    "\n",
    "# Logging\n",
    "from utils.logging import logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_models = [\n",
    "    { \n",
    "        'model_name': 'U-Net 256x256',\n",
    "        'model_path': r'checkpoints/avid-forest-323/best-checkpoint.pth.tar',\n",
    "        'patch_size': 256,\n",
    "    },\n",
    "    { \n",
    "        'model_name': 'U-Net 512x512',\n",
    "        'model_path': r'checkpoints/helpful-sky-334/best-checkpoint.pth.tar',\n",
    "        'patch_size': 512,\n",
    "    },\n",
    "   { \n",
    "        'model_name': 'U-Net 640x640',\n",
    "        'model_path': r'checkpoints/graceful-snowball-337/best-checkpoint.pth.tar',\n",
    "        'patch_size': 640,\n",
    "    }, \n",
    "    { \n",
    "        'model_name': 'U-Net 768x768',\n",
    "        'model_path': r'checkpoints/silvery-serenity-371/best-checkpoint.pth.tar',\n",
    "        'patch_size': 768,\n",
    "    },\n",
    "    { \n",
    "        'model_name': 'U-Net 800x800',\n",
    "        'model_path': r'checkpoints/kind-totem-369/best-checkpoint.pth.tar',\n",
    "        'patch_size': 800,\n",
    "    },\n",
    "    { \n",
    "        'model_name': 'U-Net 864x864',\n",
    "        'model_path': r'checkpoints/swept-field-374/best-checkpoint.pth.tar',\n",
    "        'patch_size': 864,\n",
    "    },\n",
    "    { \n",
    "        'model_name': 'U-Net 960x960',\n",
    "        'model_path': r'checkpoints/giddy-leaf-375/best-checkpoint.pth.tar',\n",
    "        'patch_size': 960,\n",
    "    },\n",
    "    { \n",
    "        'model_name': 'U-Net 1088x1088',\n",
    "        'model_path': r'checkpoints/masked-orb-376/best-checkpoint.pth.tar',\n",
    "        'patch_size': 1088,\n",
    "    },\n",
    "]\n",
    "metrics_model_index = 0\n",
    "\n",
    "metrics_output = pathlib.Path('metrics_output')\n",
    "model_metrics_output = pathlib.Path(metrics_output, metrics_models[metrics_model_index]['model_name'])\n",
    "\n",
    "# Create directory if it doesn't exists\n",
    "if not os.path.isdir(model_metrics_output):\n",
    "    os.makedirs(model_metrics_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('[DATA]: Started preloading test images and labels!')\n",
    "test_imgs = preload_image_data(r'data', r'imgs', False, metrics_models[metrics_model_index]['patch_size'])\n",
    "test_labels = preload_image_data(r'data', r'imgs', True, metrics_models[metrics_model_index]['patch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'model_name': metrics_models[metrics_model_index]['model_path'],\n",
    "    'patch_width': metrics_models[metrics_model_index]['patch_size'],\n",
    "    'patch_height': metrics_models[metrics_model_index]['patch_size'],\n",
    "    'n_channels': 3,\n",
    "    'n_classes': 3\n",
    "}\n",
    "model = Prediction(model_params)\n",
    "model.initialize()\n",
    "\n",
    "log.info('[PREDICTION]: Model loaded!')\n",
    "log.info(f'[PREDICTION]: Starting prediction on {len(test_imgs)} image(s).')\n",
    "\n",
    "predicted_labels = []\n",
    "img_process_time_list = []\n",
    "m_ious = []\n",
    "\n",
    "batch_start_time = time.time()\n",
    "pbar = tqdm.tqdm(enumerate(test_imgs), total=len(test_imgs))\n",
    "for i, img in pbar:\n",
    "    img_start_time = time.time()\n",
    "    mask_predict = model.predict_image(img)\n",
    "    img_process_time = time.time() - img_start_time\n",
    "\n",
    "    predicted_labels.append(mask_predict)\n",
    "    img_process_time_list.append(img_process_time * 1000)\n",
    "\n",
    "pbar.close()\n",
    "batch_process_time = time.time() - batch_start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(\n",
    "    save_path=model_metrics_output,\n",
    "    prefix='1',\n",
    "    image=test_imgs[8], \n",
    "    ground_truth_mask=mask2rgb(test_labels[8]), \n",
    "    predicted_mask=mask2rgb(predicted_labels[8]),\n",
    ")\n",
    "\n",
    "visualize(\n",
    "    save_path=model_metrics_output,\n",
    "    prefix='2',\n",
    "    image=test_imgs[16], \n",
    "    ground_truth_mask=mask2rgb(test_labels[16]), \n",
    "    predicted_mask=mask2rgb(predicted_labels[16]),\n",
    ")\n",
    "\n",
    "visualize(\n",
    "    save_path=model_metrics_output,\n",
    "    prefix='3',\n",
    "    image=test_imgs[32], \n",
    "    ground_truth_mask=mask2rgb(test_labels[32]), \n",
    "    predicted_mask=mask2rgb(predicted_labels[32]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall_auc(ground_truth, predicted):\n",
    "    precision, recall, _ = precision_recall_curve(ground_truth, predicted)\n",
    "    return precision, recall, auc(precision, recall)\n",
    "\n",
    "def get_classification_report(ground_truth, predicted, postfix):\n",
    "    report = classification_report(ground_truth, predicted, output_dict=True, digits = 6)\n",
    "    conf = confusion_matrix(ground_truth, predicted)\n",
    "\n",
    "    # Configure report data\n",
    "    report['1.0'].pop('support')\n",
    "    report['1.0'][f'Precision {postfix}'] = report['1.0']['precision']\n",
    "    report['1.0'][f'Recall {postfix}'] = report['1.0']['recall']\n",
    "    report['1.0'][f'F1-Score {postfix}'] = report['1.0']['f1-score']\n",
    "    report['1.0'][f'Accuracy {postfix}'] = report['accuracy']\n",
    "    report['1.0'][f'TP {postfix}'] = conf[1][1]\n",
    "    report['1.0'][f'TN {postfix}'] = conf[0][0]\n",
    "    report['1.0'][f'FP {postfix}'] = conf[0][1]\n",
    "    report['1.0'][f'FN {postfix}'] = conf[1][0]\n",
    "\n",
    "    # Calculate ROC and AUC score & ROC curve\n",
    "    auc_s = roc_auc_score(ground_truth, predicted)\n",
    "    fpr, tpr, _ = roc_curve(ground_truth, predicted)\n",
    "\n",
    "    # Return classification report only for class laber (not background)\n",
    "    return report['1.0'], auc_s, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vars\n",
    "plot_colors = ['red', 'dimgray']\n",
    "gt_list_fire = []\n",
    "pred_list_fire = []\n",
    "\n",
    "gt_list_smoke = []\n",
    "pred_list_smoke = []\n",
    "\n",
    "\n",
    "log.info('[METRICS]: Started converting RGB masks to binary masks!')\n",
    "pbar = tqdm.tqdm(enumerate(test_labels), total=len(test_labels))\n",
    "for i, label in pbar:\n",
    "    # Fire\n",
    "    ground_truth_fire = cv2.inRange(label, 1, 1)\n",
    "    prediction_fire = cv2.inRange(predicted_labels[i], 1, 1)\n",
    "\n",
    "    gt_mapped_fire = ground_truth_fire.flatten().astype('float') / 255\n",
    "    pred_mapped_fire = prediction_fire.flatten().astype('float') / 255\n",
    "\n",
    "    gt_list_fire.append(gt_mapped_fire)\n",
    "    pred_list_fire.append(pred_mapped_fire)\n",
    "\n",
    "    # Smoke\n",
    "    ground_truth_smoke = cv2.inRange(label, 2, 2)\n",
    "    prediction_smoke = cv2.inRange(predicted_labels[i], 2, 2)\n",
    "\n",
    "    gt_mapped_smoke = ground_truth_smoke.flatten().astype('float') / 255\n",
    "    pred_mapped_smoke = prediction_smoke.flatten().astype('float') / 255\n",
    "\n",
    "    gt_list_smoke.append(gt_mapped_smoke)\n",
    "    pred_list_smoke.append(pred_mapped_smoke)\n",
    "pbar.close()\n",
    "\n",
    "# Global Vars\n",
    "gt_flatten_fire, pred_flatten_fire = np.asarray(gt_list_fire).flatten(), np.asarray(pred_list_fire).flatten()\n",
    "gt_flatten_smoke, pred_flatten_smoke = np.asarray(gt_list_smoke).flatten(), np.asarray(pred_list_smoke).flatten()\n",
    "\n",
    "# Remove Unused Data From Memory\n",
    "log.info('[METRICS]: Started converting RGB masks to binary masks!')\n",
    "del test_imgs\n",
    "del test_labels\n",
    "\n",
    "# Calculate Metrics\n",
    "log.info('[METRICS]: Started calculating Precision-Recall curve!')\n",
    "precision_fire, recall_fire, auc_fire = get_precision_recall_auc(gt_flatten_fire, pred_flatten_fire)\n",
    "precision_smoke, recall_smoke, auc_smoke = get_precision_recall_auc(gt_flatten_smoke, pred_flatten_smoke)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "log.info('[METRICS]: Started plotting Precision-Recall curve!')\n",
    "plt.plot(precision_fire, recall_fire, marker='.', color=plot_colors[0], label=f'{metrics_models[metrics_model_index][\"model_name\"]} (Fire): {round(auc_fire, 3)}')\n",
    "plt.plot(precision_smoke, recall_smoke, marker='.', color=plot_colors[1], label=f'{metrics_models[metrics_model_index][\"model_name\"]} (Smoke): {round(auc_smoke, 3)}')\n",
    "\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.savefig(f\"{str(model_metrics_output.resolve())}/precision_recall_curve.svg\", format='svg', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# String log evaluation metrics\n",
    "log.info(\n",
    "f'Precision: {np.mean(precision_fire):.5f} | \\\n",
    "Recall: {np.mean(recall_fire):.5f} | \\\n",
    "AUC: {np.mean(auc_fire):.5f} | \\\n",
    "Max. Processing Time: {np.amax(img_process_time_list):.2f}ms | \\\n",
    "Avg. Processing Time: {np.mean(img_process_time_list):.2f}ms | \\\n",
    "Min. Processing Time: {np.amin(img_process_time_list):.2f}ms | \\\n",
    "Whole Processing Time: {(batch_process_time * 1000):.0f}s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report & ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info('[REPORT]: Started calculating classification report for fire!')\n",
    "report_data_fire, auc_fire, fpr_fire, tpr_fire = get_classification_report(gt_flatten_fire, pred_flatten_fire, '(fire)')\n",
    "\n",
    "log.info('[REPORT]: Started calculating classification report for smoke!')\n",
    "report_data_smoke, auc_smoke, fpr_smoke, tpr_smoke = get_classification_report(gt_flatten_smoke, pred_flatten_smoke, '(smoke)')\n",
    "\n",
    "log.info('[METRICS]: Started plotting ROC curve!')\n",
    "plt.plot(fpr_fire, tpr_fire, marker='.', color=plot_colors[0], label=f'{metrics_models[metrics_model_index][\"model_name\"]} (Fire): {round(auc_fire, 3)}')\n",
    "plt.plot(fpr_smoke, tpr_smoke, marker='.', color=plot_colors[1], linestyle='--', label=f'{metrics_models[metrics_model_index][\"model_name\"]} (Smoke): {round(auc_smoke, 3)}')\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.savefig(f\"{str(model_metrics_output.resolve())}/roc_curve.svg\", format='svg', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving report data to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "report = {}\n",
    "report_data_timings = { 'Min. Processing Time': f'{np.amin(img_process_time_list):.2f}ms', 'Max. Processing Time': f'{np.amax(img_process_time_list):.2f}ms', 'Avg. Processing Time': f'{np.mean(img_process_time_list):.2f}ms' }\n",
    "report[metrics_models[metrics_model_index]['model_name']] = report_data_fire | report_data_smoke | report_data_timings\n",
    "\n",
    "df = pd.DataFrame.from_dict(data=report, orient='index', columns=['Precision (fire)', 'Recall (fire)', 'F1-Score (fire)', 'Accuracy (fire)', 'TP (fire)', 'TN (fire)', 'FP (fire)', 'FN (fire)', 'Precision (smoke)', 'Recall (smoke)', 'F1-Score (smoke)', 'Accuracy (smoke)', 'TP (smoke)', 'TN (smoke)', 'FP (smoke)', 'FN (smoke)', 'Min. Processing Time', 'Max. Processing Time', 'Avg. Processing Time'])\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns = { 'index': 'Model Name' })\n",
    "\n",
    "if not os.path.isfile(f'{str(metrics_output.resolve())}/metrics_output.xlsx'):\n",
    "    with pd.ExcelWriter(f'{str(metrics_output.resolve())}/metrics_output.xlsx', engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name=\"PredictedResults\", index = False)\n",
    "else:\n",
    "    with pd.ExcelWriter(f'{str(metrics_output.resolve())}/metrics_output.xlsx', engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "        df.to_excel(writer, sheet_name=\"PredictedResults\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7eb2f6b845537a5839f8789b37e69e6155f5643919bcd7e9b333e352ddbc6d99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
